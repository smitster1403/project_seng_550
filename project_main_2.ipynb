{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Reddit Comments on Climate Change\n",
    "This notebook analyzes Reddit comments on climate change. Our team's goal is to: ...\n",
    "\n",
    "SENG 550 Final Project\n",
    "- Monmoy Maahdie\n",
    "- Smitkumar Saraiya\n",
    "- Farhan Ali\n",
    "- Kai Ferrer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list\n",
    "# import pyspark\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import col, length, regexp_replace, udf, split, explode\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, VectorAssembler, StopWordsRemover\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/19 15:11:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Climate Change Comments\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\")  \\\n",
    "    .config(\"spark.driver.memory\", \"4g\")  \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.num.executors\", \"4\")  \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words -= {\"no\", \"not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create dataframe\n",
    "df = spark.read.csv(\"the-reddit-climate-change-dataset-comments.csv\", header=True, inferSchema=True)\n",
    "df = df.repartition(100)  #  increase the number of partitions for large datasets - idk what to put \n",
    "# df.show(5, truncate=False) # checking the dataset by displaying first 5 rows\n",
    "df_original = df # save original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [col_name.replace('.', '_') for col_name in df.columns]\n",
    "df = df.toDF(*new_columns)\n",
    "# df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna()\n",
    "# df_clean.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(\"permalink\")\n",
    "\n",
    "# Separate records where sentiment and score can be cast to numbers\n",
    "clean_df = df_clean.filter(\n",
    "    col(\"sentiment\").cast(DoubleType()).isNotNull() &\n",
    "    col(\"score\").cast(DoubleType()).isNotNull()\n",
    ")\n",
    "\n",
    "# Records where either sentiment or score contain non-numerical values\n",
    "problematic_df = df_clean.filter(\n",
    "    col(\"sentiment\").cast(DoubleType()).isNull() |\n",
    "    col(\"score\").cast(DoubleType()).isNull()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df.filter((df_clean[\"type\"] == \"comment\") & (df_clean[\"subreddit_name\"] == \"technology\")).show(n=100, truncate=False) | YOU CAN MIDIFY THE SUBREDDIT NAME TO SEE CLIMATE CHANGE DISCUSSIONS ON DIFFERENT SUBREDDITS\n",
    "# clean_df.show(n=10, truncate=False) # we want this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problematic_df.filter(df_clean[\"type\"] == \"comment\").show(n=5, truncate=False) # this we do not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_1 = clean_df.filter(col(\"type\") == \"comment\") # only comments exist in here\n",
    "# clean_df_1.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(comment):\n",
    "    doc = nlp(comment)\n",
    "    processed_words_list = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.like_url and not token.is_stop:\n",
    "            processed_words_list.append(token.lemma_.strip().lower())\n",
    "    return ' '.join(processed_words_list)\n",
    "\n",
    "    \n",
    "# Create a UDF from the function\n",
    "preprocess_udf = udf(preprocess, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+--------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|type   |id     |subreddit_id|subreddit_name      |subreddit_nsfw|created_utc|body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |sentiment|score|processed_body                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-------+-------+------------+--------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|comment|ilzq6wi|2cn1kk      |confidentlyincorrect|false         |1661601318 |Climate change is a civilization ender, are you thick in the skull? We’re killing wild animals because we can’t keep our greed to ourselves and overpopulation is a contributor to that. Even if you don’t care about the animals, the animals are the ones keeping this planet alive, without animals our forests will die, our waters will become poison and our air will become toxic. It’s either people get rid of their greed or we have less greedy people (people in general) and the only solution is less people since nobody will sacrifice their mass consumption.                                                                                                                                                    |-0.9023  |3    |climate change civilization ender thick skull kill wild animal greed overpopulation contributor care animal animal one keep planet alive animal forest die water poison air toxic people rid greed greedy people people general solution people sacrifice mass consumption                                                                                                                                                         |\n",
      "|comment|ijh601l|2qh4r       |conspiracy          |false         |1659984481 |As oil production declines people will compete for the remaining supply driving prices up and causing a vast wealth transfer to the energy producers. By reframing the issue as climate change and charging a huge co2 tax on everything western governments can capture that wealth transfer for themselves - ostensibly to reinvest in alternative energy production and more effecient use of energy. Or maybe just to make their alt energy and green washing friends and patrons rich.                                                                                                                                                                                                                                       |0.9674   |0    |oil production decline people compete remain supply drive price cause vast wealth transfer energy producer reframe issue climate change charge huge co2 tax western government capture wealth transfer ostensibly reinvest alternative energy production effecient use energy maybe alt energy green washing friend patron rich                                                                                                    |\n",
      "|comment|ij23fxm|2r73k       |politicaldebate     |false         |1659709788 |Mc Donald's is a fast food company, you need to cook food... with a oven.... witch requires gas. Its really not concerning, even if we completely got rid of carbon emissions it wouldn't make a difference on climate change.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |-0.3612  |1    |mc donald fast food company need cook food oven witch require gas not concern completely get rid carbon emission difference climate change                                                                                                                                                                                                                                                                                         |\n",
      "|comment|igyuukm|2qh13       |worldnews           |false         |1658351775 |Climate change is the tip of the melting iceberg of things that would have been different if Gore had become president.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |0.0      |2    |climate change tip melt iceberg thing different gore president                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|comment|iioq9jv|2qndt       |republican          |false         |1659469914 |All the nonprofit organizations that claim to be objective - ACLU, SPLC, ADL - have become extremely partisan. Even tech groups like the EFF, Tor, and Mozilla are getting involved in abortion and climate change debates now.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |0.3612   |5    |nonprofit organization claim objective aclu splc adl extremely partisan tech group like eff tor mozilla getting involve abortion climate change debate                                                                                                                                                                                                                                                                             |\n",
      "|comment|ik7442o|2qh1i       |askreddit           |false         |1660438544 |\"There's only been twice in my life time that I can remember where facts have led to policy has led to action and resulted in real change globally. Y2K and CFCs in aerosol cans (ozone layer). The first one was a necessary self preservation exercise for corporations but the second one still blows me away at how universally it was adopted and so quickly resulted in reversing the damage, especially as there was very little \"\"benefit\"\" especially compared to climate change.\"                                                                                                                                                                                                                                       |-0.0828  |10   |twice life time remember fact lead policy lead action result real change globally y2 k cfcs aerosol can ozone layer necessary self preservation exercise corporation second blow away universally adopt quickly result reverse damage especially little benefit especially compare climate change                                                                                                                                  |\n",
      "|comment|imch2wv|2ql09       |pixelart            |false         |1661832131 |Good climate change comparison. Really put climate deniers in their place.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |0.1372   |1    |good climate change comparison climate denier place                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|comment|il80umt|9uno1       |murderbywords       |false         |1661106433 |I think that they did.  It's because of idiots who think like this that millions - of not billions - of people could die as a result of climate change.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |-0.34    |1    |think  idiot think like million not billion people die result climate change                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|comment|ikaldzx|3fuwj       |askthe_donald       |false         |1660507425 |Man made climate change is an absolute hoax. The richest people get behind the scam to gain access to the most prime real estate. Some fool believing this fraudulent theory sold their exclusive homes, only to have the proprietor of the hoax come in and buy them out. I don’t know why people keep believing these snake oil selling, carpet baggers. The national weather service cannot reliably, or accurately predict the weather coming in two days, let alone someone saying what it’s going to be like in 50 years.                                                                                                                                                                                                   |-0.6369  |13   |man climate change absolute hoax rich people scam gain access prime real estate fool believe fraudulent theory sell exclusive home proprietor hoax come buy know people believe snake oil selling carpet bagger national weather service not reliably accurately predict weather come day let say go like 50 year                                                                                                                  |\n",
      "|comment|iix1mao|2so8u       |thebrewery          |false         |1659622018 |The 2021 grain harvest was pretty bad quality. Kernels didn't ripen well and protein content was really high if I'm remembering correctly. With climate change this may become a trend along with the occasional crop failure becoming more and more common. Water shortages and rationing will also become more common and brewing will likely take hits, potentially shutting down breweries as they lose profitably and owners cash out. As much as we may hate to admit it beer, especially craft beer, is a luxury item and it will likely die off as resources become scarce. Also the working conditions in breweries are not super comfortable in heat waves and the pay is typically sub-par for a physically taxing job.|-0.9592  |7    |2021 grain harvest pretty bad quality kernels ripen protein content high remember correctly climate change trend occasional crop failure common water shortage rationing common brewing likely hit potentially shut brewery lose profitably owner cash hate admit beer especially craft beer luxury item likely die resource scarce work condition brewery not super comfortable heat wave pay typically sub par physically tax job|\n",
      "+-------+-------+------------+--------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the UDF to create a new column\n",
    "clean_df_2 = clean_df_1.withColumn('processed_body', preprocess_udf(col('body')))\n",
    "clean_df_2.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(sentiment):\n",
    "    if float(sentiment) < -0.05:\n",
    "        return -1\n",
    "    elif float(sentiment) >= -0.05 and float(sentiment) <= 0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "create_label_udf = udf(create_label, IntegerType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_3 = clean_df_2.withColumn('label', create_label_udf(col('sentiment')))\n",
    "# Count occurrences of each subreddit_name and order by count in descending order\n",
    "subreddit_counts = clean_df_3.groupBy('subreddit_name') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False)\n",
    "\n",
    "# Show the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Most common subreddits:\")\n",
    "# subreddit_counts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you need the total number of unique subreddits\n",
    "# unique_subreddits = subreddit_counts.count()\n",
    "# print(f\"\\nTotal number of unique subreddits: {unique_subreddits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_counts.filter(subreddit_counts['subreddit_name'] == 'climate').show()\n",
    "clean_df_3 = clean_df_3.withColumn(\"sentiment\", col(\"sentiment\").cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"At tokenizer\")\n",
    "# # Tokenize comment text\n",
    "# tokenizer = Tokenizer(inputCol=\"processed_body\", outputCol=\"words\")\n",
    "\n",
    "# # Transform words into numerical features\n",
    "# print(\"At hashingTF\")\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=10000)\n",
    "\n",
    "# # Define the model\n",
    "# print(\"At linear regression\")\n",
    "# lr = LinearRegression(featuresCol=\"features\", labelCol=\"sentiment\")\n",
    "\n",
    "# # Create a pipeline\n",
    "# print(\"At pipeline\")\n",
    "# pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# # Split the data\n",
    "# print(\"Data split\")\n",
    "# (train_data, test_data) = clean_df_3.randomSplit([0.01, 0.01])\n",
    "\n",
    "# # Train the model\n",
    "# print(\"model traiing\")\n",
    "# model = pipeline.fit(train_data)\n",
    "\n",
    "# # Make predictions\n",
    "# print(\"Predictions\")\n",
    "# predictions = model.transform(test_data)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Mordel eval\")\n",
    "# evaluator = RegressionEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+--------------------+--------------+-----------+--------------------+---------+-----+--------------------+-----+\n",
      "|   type|     id|subreddit_id|      subreddit_name|subreddit_nsfw|created_utc|                body|sentiment|score|      processed_body|label|\n",
      "+-------+-------+------------+--------------------+--------------+-----------+--------------------+---------+-----+--------------------+-----+\n",
      "|comment|ilzq6wi|      2cn1kk|confidentlyincorrect|         false| 1661601318|Climate change is...|  -0.9023|    3|climate change ci...|   -1|\n",
      "|comment|ijh601l|       2qh4r|          conspiracy|         false| 1659984481|As oil production...|   0.9674|    0|oil production de...|    1|\n",
      "|comment|ij23fxm|       2r73k|     politicaldebate|         false| 1659709788|Mc Donald's is a ...|  -0.3612|    1|mc donald fast fo...|   -1|\n",
      "+-------+-------+------------+--------------------+--------------+-----------+--------------------+---------+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[type: string, id: string, subreddit_id: string, subreddit_name: string, subreddit_nsfw: string, created_utc: string, body: string, sentiment: double, score: string, processed_body: string, label: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_3.show(3)\n",
    "clean_df_3.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"134217728\")  # 128MB per partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce the DataFrame to a smaller number of partitions\n",
    "clean_df_3 = clean_df_3.coalesce(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[type: string, id: string, subreddit_id: string, subreddit_name: string, subreddit_nsfw: string, created_utc: string, body: string, sentiment: double, score: string, processed_body: string, label: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_3.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.              (0 + 10) / 100]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclean_df_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:================>                                     (30 + 10) / 100]\r"
     ]
    }
   ],
   "source": [
    "clean_df_3.write.saveAsTable(\"cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM cleaned\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Warehouse\n",
    "\n",
    "just using whatever Kai put down for spark warehousing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE reddit_db\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show() # check that reddit_db is in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show() # should be empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS reddit_db.comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS reddit_db.comments (\n",
    "    `type` STRING,\n",
    "    `id` STRING,\n",
    "    `subreddit_id` STRING,\n",
    "    `subreddit_name` STRING,\n",
    "    `subreddit_nsfw` STRING,\n",
    "    `created_utc` STRING,\n",
    "    `body` STRING,\n",
    "    `sentiment` STRING,\n",
    "    `score` STRING,\n",
    "    `processed_body` STRING,\n",
    "    `label` STRING\n",
    ")\n",
    "USING PARQUET\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE reddit_db\") # ensure youre on the right db\n",
    "spark.sql(\"SHOW TABLES\").show() # should be updated to have one table now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_3.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the columns - spark only accepts '_' but the dataset uses '.'\n",
    "df_aligned = clean_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned.printSchema() # double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE reddit_db\") # ensure youre on the right db\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df_aligned.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have a large dataset of 1mil+ entries, start by repartitioning the data\n",
    "#df_aligned_repartitioned = df_aligned.repartition(20)  # The number can vary based on your machine, but we recommend keeping it from 20-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition_sizes = df_aligned.rdd.glom().map(len).collect()\n",
    "# print(partition_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coalesced = df_aligned.coalesce(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DataFrame into smaller batches and insert into the table\n",
    "df_coalesced = df_aligned.coalesce(10)  # Reduce to fewer partitions\n",
    "batch_size = df_coalesced.count() // 10  # Divide into 10 batches\n",
    "\n",
    "for i in range(10):\n",
    "    df_batch = df_coalesced.limit(batch_size)\n",
    "    df_batch.write.insertInto(\"reddit_db.comments\", overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coalesced.createOrReplaceTempView(\"temp_df\")  # Create a temporary view of the DataFrame\n",
    "\n",
    "# Use Spark SQL to insert into the table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT OVERWRITE TABLE reddit_db.comments\n",
    "    SELECT * FROM temp_df\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_coalesced.write.insertInto(\"reddit_db.comments\", overwrite=True)\n",
    " # insert data from csv/df into spark table\n",
    "#df_coalesced.write.mode(\"overwrite\").parquet(\"/spark-warehouse/reddit_db.db/comments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM reddit_db.comments LIMIT 100\").show() #validate the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = df_aligned.withColumn(\"words\", split(col(\"body\"), r\"\\s+\"))\n",
    "df_tokens = df_tokens.filter(df_tokens[\"words\"].isNotNull())\n",
    "df_tokens.show(5) #check if words column created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned_words = remover.transform(df_tokens)\n",
    "df_aligned_words.show(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode - helps so that each word appears in a separate row so we can count frequency\n",
    "df_exploded = df_aligned_words.withColumn(\"word\", explode(col(\"filtered_words\")))\n",
    "df_exploded.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_count = df_exploded.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "df_word_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
