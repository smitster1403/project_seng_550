{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Reddit Comments on Climate Change\n",
    "This notebook analyzes Reddit comments on climate change. Our team's goal is to: ...\n",
    "\n",
    "SENG 550 Final Project\n",
    "- Monmoy Maahdie\n",
    "- Smitkumar Saraiya\n",
    "- Farhan LASTNAME\n",
    "- Kai Ferrer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary requirements:\n",
    "- Download The Reddit Climate Change Dataset (https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset?resource=download) and add to your root directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an ApacheSpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "# import pyspark\n",
    "# from collections import Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reddit Climate Change Comments\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df = spark.read.csv(\"the-reddit-climate-change-dataset-comments.csv\", header=True, inferSchema=True)\n",
    "df = df.repartition(100)  #  increase the number of partitions for large datasets - idk what to put \n",
    "df.show(5, truncate=False) # checking the dataset by displaying first 5 rows\n",
    "df_original = df # save original dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "The dataset will be cleaned by\n",
    "- Renaming columns \n",
    "- Removing rows with NULL values\n",
    "- ???\n",
    "- ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the column names first because they use 'subreddit.id' and use a period\n",
    "# for better readability we want to change it to \"subreddit_id\", etc.\n",
    "new_columns = [col_name.replace('.', '_') for col_name in df.columns]\n",
    "df = df.toDF(*new_columns)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NULL values\n",
    "df_clean = df.dropna()\n",
    "df_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count1 = df_clean.count() \n",
    "print(f\"Cleaned dataset rows: {row_count1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix data types that are incorrect (?)\n",
    "There's an issue where basically the \"body\" feeds into the other columns. See output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:===================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                type|\n",
      "+--------------------+\n",
      "|&gt; The scientis...|\n",
      "|Joe is weird with...|\n",
      "|I agree with that...|\n",
      "|I'm glad there's ...|\n",
      "|Those companies y...|\n",
      "|If we also reduce...|\n",
      "|What I think I am...|\n",
      "|  As for Indian food|\n",
      "|https://www.theve...|\n",
      "|&gt; We were prom...|\n",
      "|Markets are prett...|\n",
      "|Bike infrastructu...|\n",
      "|I don't mean that...|\n",
      "|Just look at a gr...|\n",
      "|The comment I’m r...|\n",
      "|               Yep.\"|\n",
      "|[Here are some id...|\n",
      "|&gt;They will die...|\n",
      "|            Normally|\n",
      "| Isn't there envy...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean.select(\"type\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate Data (?) - cant really do much until the top is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of duplicates\n",
    "df_clean.groupBy(\"id\").count().filter(\"count > 1\").show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on columns that have the same inputs in 'id' (this would be the most unique)\n",
    "df_clean = df_clean.dropDuplicates([\"id\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count2 = df_clean.count() \n",
    "print(f\"Dropped dataset rows: {row_count2}\") # Check updated if any duplicates were rlly dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('the-reddit-climate-change-dataset-comments.csv')\n",
    "# data = data.dropna() # drop any rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter(data['subreddit.nsfw'])\n",
    "# print(counter)\n",
    "# print(data['score'])\n",
    "#print(data['body'].iloc[20340])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Transformation \n",
    "Not sure yet what to do here idk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data of type \"comment\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_clean.filter(df_clean[\"type\"] == \"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 289:=================================================>    (91 + 9) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+--------------------+--------------+-----------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "|type   |id     |subreddit_id|subreddit_name      |subreddit_nsfw|created_utc|permalink                                                                                                        |body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |sentiment|score|\n",
      "+-------+-------+------------+--------------------+--------------+-----------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "|comment|ihpmnzr|2t7no       |futurology          |false         |1658841679 |https://old.reddit.com/r/Futurology/comments/w8eofy/us_to_plant_1_billion_trees_as_climate_change/ihpmnzr/       |Hate to say it, but those trees are going to catch fire again anyways unless we do something about climate change.                                                                                                                                                                                                                                                                                                                                                                                                                                            |-0.6652  |1    |\n",
      "|comment|ihcq4vl|3f8vd       |canadapublicservants|false         |1658601762 |https://old.reddit.com/r/CanadaPublicServants/comments/w5a1fc/weekly_megathread_wfh_and_returntooffice/ihcq4vl/  |Although HC released a report on climate change, we were told that PSPC needs to substantiate the environmental impact of BTO. Such BS.                                                                                                                                                                                                                                                                                                                                                                                                                       |0.0      |5    |\n",
      "|comment|ii9uptv|2qh1u       |music               |false         |1659198393 |https://old.reddit.com/r/Music/comments/wc0201/taylor_swifts_private_jets_took_170_trips_this/ii9uptv/           |Yes we can resolve the climate crisis by making celebrities stop flying private. Yes this is the end solution. My god you guys have figured it out. This is akin to saying you wont drink a cup of $2 coffee per day because you are saving money to buy a house. If you guys are truly after resolving issues related to pollution and climate change then start with the big polluters as the effort will reap a lot more benefits for the planet than trying to convince maybe a few rich celebritied to stop flying private. This is all virtue signaling.|0.9402   |-23  |\n",
      "|comment|ijwnlc7|2xxyj       |damnthatsinteresting|false         |1660250686 |https://old.reddit.com/r/Damnthatsinteresting/comments/wlql19/the_longest_river_in_france_dried_up_today/ijwnlc7/|something something climate change isn't real something something people don't want to work these days                                                                                                                                                                                                                                                                                                                                                                                                                                                        |-0.0572  |0    |\n",
      "|comment|ihy1woj|2qh13       |worldnews           |false         |1658979005 |https://old.reddit.com/r/worldnews/comments/w9hxg4/zoe_becomes_the_worlds_first_named_heat_wave/ihy1woj/         |Is this just them accepting that we’re gonna have record heat waves every year? “Well we’ve just accepted that the war on climate change is lost 🤷‍♂️”                                                                                                                                                                                                                                                                                                                                                                                                       |-0.3612  |1    |\n",
      "+-------+-------+------------+--------------------+--------------+-----------+-----------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_comments.show(5, truncate=False) # intermediary check to see if the filtered work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows with type \"comment\"\n",
    "print(f\"Number of rows with type 'comment': {df_comments.count()}\") # check to see if any rows were actually removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Spark tables\n",
    "idk if we need to do this really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark database\n",
    "# NOTE: UNCOMMENT IF NOT CREATED YET, OTHERWISE COMMENT OUT IF CREATED ALREADY\n",
    "spark.sql(\"CREATE DATABASE reddit_db\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show() # check that reddit_db is in here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show() # should be empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if it already exists\n",
    "# spark.sql(\"USE reddit_db\")\n",
    "# df_filtered.write.mode(\"overwrite\").saveAsTable(\"reddit_db.comments\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS reddit_db.comments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS reddit_db.comments (\n",
    "    `type` STRING,\n",
    "    `id` STRING,\n",
    "    `subreddit.id` STRING,\n",
    "    `subreddit.name` STRING,\n",
    "    `subreddit.nsfw` STRING,\n",
    "    `created_utc` STRING,\n",
    "    `permalink` STRING,\n",
    "    `body` STRING,\n",
    "    `sentiment` STRING,\n",
    "    `score` STRING\n",
    ")\n",
    "USING PARQUET\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show() # should be updated to have one table now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the columns - spark only accepts '_' but the dataset uses '.'\n",
    "df_aligned = df_comments \\\n",
    "    .withColumnRenamed(\"subreddit.id\", \"subreddit_id\") \\\n",
    "    .withColumnRenamed(\"subreddit.name\", \"subreddit_name\") \\\n",
    "    .withColumnRenamed(\"subreddit.nsfw\", \"subreddit_nsfw\")\n",
    "\n",
    "# this is lowkey still transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned.printSchema() # double check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned.write.insertInto(\"reddit_db.comments\", overwrite=False) # insert data from csv/df into spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM reddit_db.comments LIMIT 5\").show() #validate the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = df_aligned.withColumn(\"words\", split(col(\"body\"), r\"\\s+\"))\n",
    "df_tokens = df_tokens.filter(df_tokens[\"words\"].isNotNull())\n",
    "df_tokens.show(5) #check if words column created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init stopwordsremover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned_words = remover.transform(df_tokens)\n",
    "df_aligned_words.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode - helps so that each word appears in a separate row so we can count frequency\n",
    "df_exploded = df_aligned_words.withColumn(\"word\", explode(col(\"filtered_words\")))\n",
    "df_exploded.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_count = df_exploded.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "df_word_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
